{
  "userInputs": [
    {
      "id": "6573d9ac-75c4-4a5f-a9a2-2703f28402e8",
      "type": "url",
      "name": "Enter your website or content URL",
      "description": "The URL to generate Q&A content from",
      "placeholder": "https://frevana.com/",
      "required": true
    }
  ],
  "inputValues": {
    "6573d9ac-75c4-4a5f-a9a2-2703f28402e8": ["https://www.frevana.com"]
  },
  "workflows": [
    {
      "id": "0d679e19-c80f-44dd-8aba-7b8eca3e56ff",
      "nameV2": "Web scraping",
      "type": "fetch_web",
      "outputFormat": "text",
      "prompt": "@[Question-1 Enter the URL](6573d9ac-75c4-4a5f-a9a2-2703f28402e8)"
    },
    {
      "id": "b7c8d9e0-2f3a-4b5c-a6d7-e8f9a0b1c2d3",
      "nameV2": "Extract Q&A data",
      "type": "ai_prompt",
      "outputFormat": "text",
      "model": "gpt-5.1",
      "prompt": "You are an expert content analyzer and Q&A generator. Analyze the provided web content and extract comprehensive Q&A pairs for a FAQ landing page.\n\nWeb Content:\n{{ @[Step-1 Web scraping](0d679e19-c80f-44dd-8aba-7b8eca3e56ff) }}\n\n## CRITICAL DATA PRESERVATION REQUIREMENTS:\n- PRESERVE ALL original content, company information, and specific details from the URL\n- DO NOT remove, modify, or simplify any factual information\n- DO NOT change specific company names, product names, or technical terms\n- MAINTAIN all original URLs, links, and references exactly as provided\n- KEEP all business information, features, and capabilities unchanged\n- PRESERVE all contact information and support details\n\n## EXTRACTION REQUIREMENTS:\nGenerate 15-25 comprehensive Q&A pairs covering:\n1. Company overview and philosophy\n2. Product/service features and benefits\n3. Technical capabilities and specifications\n4. Pricing and business models\n5. Getting started and onboarding\n6. Support and contact information\n7. Security and data protection\n8. Target audience and use cases\n9. Integration and compatibility\n10. Performance and scalability\n\n## OUTPUT FORMAT:\nReturn the data as a CSV with the following structure:\n```csv\nquestion,answer,category,priority\n\"Question text here\",\"Detailed answer here\",\"Category name\",1\n```\n\n### CSV Guidelines:\n- Use proper CSV escaping (quotes around fields containing commas, newlines, or quotes)\n- Each answer should be 100-300 words\n- Category should be one of: General, Features, Integration, Pricing, Support, Security, Technical\n- Priority: 1 (high), 2 (medium), 3 (low)\n- Include header row\n- Escape double quotes by doubling them (\"\" for \")\n\n### Example:\n```csv\nquestion,answer,category,priority\n\"What is ProductName?\",\"ProductName is a comprehensive solution that helps businesses...\",\"General\",1\n\"How does the pricing work?\",\"Our pricing model is designed to be flexible...\",\"Pricing\",2\n```\n\nOutput ONLY the CSV data without any markdown code blocks or additional text."
    },
    {
      "id": "c9d0e1f2-3a4b-5c6d-a7e8-f9a0b1c2d3e4",
      "nameV2": "Deduplicate and save to CSV",
      "type": "python_script",
      "outputFormat": "csv",
      "nodeIds": ["0d679e19-c80f-44dd-8aba-7b8eca3e56ff", "b7c8d9e0-2f3a-4b5c-a6d7-e8f9a0b1c2d3"],
      "script": "# Python script for CSV deduplication and incremental saving\nimport csv\nimport os\nimport hashlib\nfrom io import StringIO\nfrom datetime import datetime\n\ndef normalize_text(text):\n    \"\"\"Normalize text for comparison by removing extra whitespace and lowercasing\"\"\"\n    return ' '.join(text.lower().strip().split())\n\ndef generate_hash(question, answer):\n    \"\"\"Generate a unique hash for a Q&A pair\"\"\"\n    combined = f\"{normalize_text(question)}|{normalize_text(answer)}\"\n    return hashlib.md5(combined.encode('utf-8')).hexdigest()\n\ndef main():\n    # Get the new CSV data from the AI extraction step\n    new_csv_data = data.get('b7c8d9e0-2f3a-4b5c-a6d7-e8f9a0b1c2d3', '').strip()\n    \n    if not new_csv_data:\n        return \"Error: No CSV data generated from extraction step\"\n    \n    # Parse the new CSV data\n    new_rows = []\n    existing_hashes = set()\n    \n    try:\n        csv_reader = csv.DictReader(StringIO(new_csv_data))\n        for row in csv_reader:\n            question = row.get('question', '').strip()\n            answer = row.get('answer', '').strip()\n            category = row.get('category', 'General').strip()\n            priority = row.get('priority', '2').strip()\n            \n            if question and answer:\n                qa_hash = generate_hash(question, answer)\n                \n                if qa_hash not in existing_hashes:\n                    existing_hashes.add(qa_hash)\n                    new_rows.append({\n                        'question': question,\n                        'answer': answer,\n                        'category': category,\n                        'priority': priority,\n                        'hash': qa_hash,\n                        'created_at': datetime.now().isoformat()\n                    })\n    except Exception as e:\n        return f\"Error parsing CSV data: {str(e)}\"\n    \n    # Check if CSV file exists and load existing data\n    csv_file_path = os.path.join(os.getcwd(), 'qa_database.csv')\n    existing_rows = []\n    \n    if os.path.exists(csv_file_path):\n        try:\n            with open(csv_file_path, 'r', encoding='utf-8') as f:\n                reader = csv.DictReader(f)\n                for row in reader:\n                    existing_hash = row.get('hash', '')\n                    if existing_hash:\n                        existing_hashes.add(existing_hash)\n                        existing_rows.append(row)\n        except Exception as e:\n            print(f\"Warning: Error reading existing CSV: {str(e)}\")\n    \n    # Filter out duplicates from new rows\n    unique_new_rows = []\n    duplicate_count = 0\n    \n    for row in new_rows:\n        if row['hash'] not in existing_hashes:\n            unique_new_rows.append(row)\n            existing_hashes.add(row['hash'])\n        else:\n            duplicate_count += 1\n    \n    # Combine existing and new unique rows\n    all_rows = existing_rows + unique_new_rows\n    \n    # Save to CSV file\n    if all_rows:\n        try:\n            with open(csv_file_path, 'w', encoding='utf-8', newline='') as f:\n                fieldnames = ['question', 'answer', 'category', 'priority', 'hash', 'created_at']\n                writer = csv.DictWriter(f, fieldnames=fieldnames)\n                writer.writeheader()\n                writer.writerows(all_rows)\n        except Exception as e:\n            return f\"Error writing CSV file: {str(e)}\"\n    \n    # Generate summary report\n    summary = f\"\"\"CSV Deduplication Summary:\n- Total new Q&A pairs extracted: {len(new_rows)}\n- Unique new Q&A pairs added: {len(unique_new_rows)}\n- Duplicates skipped: {duplicate_count}\n- Total Q&A pairs in database: {len(all_rows)}\n- CSV file saved to: {csv_file_path}\n\"\"\"\n    \n    return summary\n\n# Execute the main function and print the result\nif __name__ == \"__main__\":\n    output = main()\n    print(output)\n"
    },
    {
      "id": "d0e1f2a3-4b5c-6d7e-a8f9-b0c1d2e3f4a5",
      "nameV2": "Load CSV data for HTML generation",
      "type": "python_script",
      "outputFormat": "text",
      "nodeIds": ["c9d0e1f2-3a4b-5c6d-a7e8-f9a0b1c2d3e4"],
      "script": "# Python script to load CSV data and format it for HTML template\nimport csv\nimport os\nimport json\n\ndef main():\n    csv_file_path = os.path.join(os.getcwd(), 'qa_database.csv')\n    \n    if not os.path.exists(csv_file_path):\n        return json.dumps({\n            'error': 'CSV file not found',\n            'qa_pairs': []\n        })\n    \n    qa_pairs = []\n    categories = {}\n    \n    try:\n        with open(csv_file_path, 'r', encoding='utf-8') as f:\n            reader = csv.DictReader(f)\n            for row in reader:\n                question = row.get('question', '').strip()\n                answer = row.get('answer', '').strip()\n                category = row.get('category', 'General').strip()\n                priority = int(row.get('priority', '2'))\n                \n                if question and answer:\n                    qa_pair = {\n                        'question': question,\n                        'answer': answer,\n                        'category': category,\n                        'priority': priority\n                    }\n                    qa_pairs.append(qa_pair)\n                    \n                    # Group by category\n                    if category not in categories:\n                        categories[category] = []\n                    categories[category].append(qa_pair)\n    except Exception as e:\n        return json.dumps({\n            'error': f'Error reading CSV: {str(e)}',\n            'qa_pairs': []\n        })\n    \n    # Sort Q&A pairs by priority (1 = high priority first)\n    qa_pairs.sort(key=lambda x: (x['priority'], x['question']))\n    \n    # Sort categories\n    for category in categories:\n        categories[category].sort(key=lambda x: (x['priority'], x['question']))\n    \n    # Format the output as JSON for easy consumption by HTML template\n    result = {\n        'total_count': len(qa_pairs),\n        'qa_pairs': qa_pairs,\n        'categories': categories,\n        'category_list': list(categories.keys())\n    }\n    \n    return json.dumps(result, ensure_ascii=False, indent=2)\n\n# Execute the main function and print the result\nif __name__ == \"__main__\":\n    output = main()\n    print(output)\n"
    }
  ],
  "results": [
    {
      "type": "csv",
      "fields": [],
      "items": {
        "content": "c9d0e1f2-3a4b-5c6d-a7e8-f9a0b1c2d3e4"
      }
    },
    {
      "type": "html",
      "fields": ["content"],
      "items": {
        "content": "d0e1f2a3-4b5c-6d7e-a8f9-b0c1d2e3f4a5"
      },
      "metadata": {
        "templateId": "qa-page-style-1"
      }
    }
  ],
  "version": "1.0",
  "metadata": {
    "author": "System",
    "createdAt": "2026-01-30T08:00:00.000Z",
    "description": "Enhanced Q&A page generator with CSV storage and incremental deduplication"
  },
  "frequency": "-1",
  "workflowConnects": []
}
