{
  "userInputs": [
    {
      "id": "6573d9ac-75c4-4a5f-a9a2-2703f28402e8",
      "type": "url",
      "name": "Enter your website or content URL",
      "description": "",
      "placeholder": "https://frevana.com/",
      "required": true
    }
  ],
  "inputValues": {
    "6573d9ac-75c4-4a5f-a9a2-2703f28402e8": ["https://www.frevana.com"]
  },
  "workflows": [
    {
      "id": "0d679e19-c80f-44dd-8aba-7b8eca3e56ff",
      "nameV2": "Web scraping",
      "type": "fetch_web",
      "outputFormat": "text",
      "prompt": "@[Question-1 Enter the URL](6573d9ac-75c4-4a5f-a9a2-2703f28402e8) "
    },
    {
      "id": "1a2b3c4d-5e6f-7g8h-9i0j-1k2l3m4n5o6p",
      "nameV2": "Generate Q&A from Content",
      "type": "ai_prompt",
      "outputFormat": "text",
      "prompt": "Based on the provided URL content from @[Step-1 Web scraping](0d679e19-c80f-44dd-8aba-7b8eca3e56ff), analyze the content and generate a comprehensive Q&A list.\n\n## YOUR TASK:\n1. ANALYZE the provided URL content thoroughly\n2. EXTRACT key information about the company/product/service\n3. GENERATE comprehensive Q&A pairs covering:\n   - Company overview and philosophy\n   - Product/service features and benefits\n   - Technical capabilities and specifications\n   - Pricing and business models\n   - Getting started and onboarding\n   - Support and contact information\n   - Security and data protection\n   - Target audience and use cases\n   - Integration and compatibility\n   - Performance and scalability\n\n## CRITICAL REQUIREMENTS:\n- Generate 12-25 Q&A pairs minimum\n- Each answer should be detailed and informative (100-300 words)\n- Use ONLY information from the provided URL content\n- Do NOT hallucinate or make up information\n- Preserve company names, product names, and URLs exactly as provided\n\n## OUTPUT FORMAT:\nReturn a JSON array of Q&A pairs:\n[\n  {\"question\": \"What is [Company/Product Name]?\", \"answer\": \"Detailed answer here...\"},\n  {\"question\": \"What are the key features?\", \"answer\": \"Detailed answer here...\"},\n  ...\n]\n\nReturn ONLY the JSON array, no extra text or markdown formatting.",
      "model": "gpt-4.1"
    },
    {
      "id": "2b3c4d5e-6f7g-8h9i-0j1k-2l3m4n5o6p7q",
      "nameV2": "Extract Q&A List",
      "type": "ai_prompt",
      "outputFormat": "text",
      "prompt": "Extract and validate the Q&A pairs from @[Step-2 Generate Q&A from Content](1a2b3c4d-5e6f-7g8h-9i0j-1k2l3m4n5o6p)\n\nIMPORTANT: Preserve the EXACT original wording of each question. Do NOT rephrase, paraphrase, or modify the question text in any way.\n\nOutput format: JSON array with question and answer pairs. Each answer should be concise and factual.\n\nExample output:\n[\n  {\"question\": \"What is the battery life?\", \"answer\": \"The battery lasts up to 10 hours on a single charge.\"},\n  {\"question\": \"Is it compatible with Alexa?\", \"answer\": \"Yes, it works with Alexa, Google Home, and Siri.\"}\n]\n\nReturn ONLY the JSON array, no extra text.",
      "model": "gpt-4.1-mini"
    },
    {
      "id": "3c4d5e6f-7g8h-9i0j-1k2l-3m4n5o6p7q8r",
      "nameV2": "Export Q&A to CSV",
      "type": "python_script",
      "outputFormat": "text",
      "script": "# Python script for workflow task - Export Q&A to CSV\n# Available global variables:\n# - data: dict containing results from previous nodes\n\nimport json\nimport csv\nimport os\nfrom datetime import datetime\n\ndef parse_qa_pairs(qa_json_str, current_time):\n    \"\"\"\n    Parse Q&A pairs from JSON string.\n    \n    Args:\n        qa_json_str: JSON string containing Q&A pairs\n        current_time: Current datetime string to assign to new records\n        \n    Returns:\n        List of tuples (Time, Question, Answer)\n    \"\"\"\n    if not qa_json_str:\n        return []\n    \n    try:\n        # Try to parse as JSON array\n        qa_pairs = json.loads(qa_json_str)\n        if not isinstance(qa_pairs, list):\n            return []\n        \n        result = []\n        for item in qa_pairs:\n            if isinstance(item, dict) and 'question' in item and 'answer' in item:\n                question = item['question'].strip()\n                answer = item['answer'].strip()\n                if question and answer:\n                    result.append((current_time, question, answer))\n        \n        return result\n    except json.JSONDecodeError:\n        # If JSON parsing fails, return empty list\n        return []\n    except Exception as e:\n        return []\n\ndef read_existing_csv(csv_path):\n    \"\"\"\n    Read existing CSV file and return list of (Time, Question, Answer) tuples.\n    \n    Args:\n        csv_path: Path to CSV file\n        \n    Returns:\n        List of (Time, Question, Answer) tuples\n    \"\"\"\n    existing_records = []\n    \n    if os.path.exists(csv_path):\n        try:\n            with open(csv_path, 'r', encoding='utf-8', newline='') as f:\n                reader = csv.DictReader(f)\n                for row in reader:\n                    if 'Question' in row and row['Question']:\n                        dt = row.get('Time', '').strip()\n                        question = row['Question'].strip()\n                        answer = row.get('Answer', '').strip()\n                        existing_records.append((dt, question, answer))\n        except Exception as e:\n            # If there's an error reading the file, just return empty list\n            pass\n    \n    return existing_records\n\ndef export_to_csv(qa_data, csv_path):\n    \"\"\"\n    Export Q&A to CSV file with deduplication.\n    Rewrites the entire file with existing + new records.\n    \n    Args:\n        qa_data: List of tuples (Time, Question, Answer)\n        csv_path: Path to CSV file\n        \n    Returns:\n        dict with export statistics\n    \"\"\"\n    # Read existing records\n    existing_records = read_existing_csv(csv_path)\n    \n    # Create a set of questions for quick duplicate checking\n    existing_questions = {question for dt, question, answer in existing_records}\n    \n    # Merge existing and new records, filtering duplicates\n    all_records = list(existing_records)\n    new_count = 0\n    duplicate_count = 0\n    \n    for dt, question, answer in qa_data:\n        if question not in existing_questions:\n            # New record, add it\n            all_records.append((dt, question, answer))\n            existing_questions.add(question)\n            new_count += 1\n        else:\n            # Duplicate, skip it\n            duplicate_count += 1\n    \n    # Sort records by time, then by question\n    sorted_records = sorted(all_records, key=lambda x: (x[0], x[1]))\n    \n    # Rewrite entire CSV file\n    try:\n        # Ensure directory exists\n        os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n        \n        with open(csv_path, 'w', encoding='utf-8', newline='') as f:\n            writer = csv.writer(f)\n            \n            # Write header with Answer column\n            writer.writerow(['Time', 'Question', 'Answer'])\n            \n            # Write all records\n            for dt, question, answer in sorted_records:\n                writer.writerow([dt, question, answer])\n        \n        return {\n            'success': True,\n            'message': f'Successfully exported {new_count} new Q&A ({duplicate_count} duplicates skipped). Total records: {len(sorted_records)}',\n            'data': csv_path\n        }\n        \n    except Exception as e:\n        return {\n            'success': False,\n            'message': f'Failed to export Q&A: {str(e)}',\n            'data': None\n        }\n\ndef main():\n    \"\"\"\n    Main function to process Q&A and export to CSV.\n    \"\"\"\n    try:\n        # Check data exists\n        if 'data' not in globals() or not isinstance(data, dict):\n            return json.dumps({\n                \"success\": False,\n                \"message\": \"Data not found\",\n                \"data\": None\n            }, indent=2)\n        \n        # Get CSV directory and filename from data\n        csv_directory = get_agent_dir()\n            \n        # Construct CSV path\n        csv_filename = 'Website Q&A Research.csv'\n        csv_path = os.path.join(csv_directory, csv_filename)\n        \n        # Get current time with minute precision\n        current_time = datetime.now().strftime('%Y-%m-%d %H:%M')\n        \n        # Get input data - Q&A JSON from Step 3\n        qa_json_str = data.get('2b3c4d5e-6f7g-8h9i-0j1k-2l3m4n5o6p7q', '').strip()\n        \n        # Parse Q&A pairs\n        all_qa = parse_qa_pairs(qa_json_str, current_time)\n        \n        # Export to CSV\n        if all_qa:\n            result = export_to_csv(all_qa, csv_path)\n            return json.dumps({\n                \"success\": result['success'],\n                \"message\": result['message'],\n                \"data\": result['data']\n            }, indent=2)\n        else:\n            return json.dumps({\n                \"success\": False,\n                \"message\": \"No Q&A found in input data\",\n                \"data\": csv_path\n            }, indent=2)\n        \n    except Exception as e:\n        return json.dumps({\n            \"success\": False,\n            \"message\": f\"Error: {str(e)}\",\n            \"data\": None\n        }, indent=2)\n\nif __name__ == \"__main__\":\n    output = main()\n    print(output)\n",
      "nodeIds": ["2b3c4d5e-6f7g-8h9i-0j1k-2l3m4n5o6p7q"]
    },
    {
      "id": "4d5e6f7g-8h9i-0j1k-2l3m-4n5o6p7q8r9s",
      "nameV2": "Get CSV file path",
      "type": "script",
      "outputFormat": "text",
      "script": "\nmodule.exports = async function() {\n  \n  let result = JSON.parse(data[\"3c4d5e6f-7g8h-9i0j-1k2l-3m4n5o6p7q8r\"]);\n  if(result.success && !!result.data){\n    return result.data;\n  }\n  throw Error(result?.message || \"Export Q&A error\");\n  \n}\n",
      "nodeIds": ["3c4d5e6f-7g8h-9i0j-1k2l-3m4n5o6p7q8r"]
    },
    {
      "id": "5e6f7g8h-9i0j-1k2l-3m4n-5o6p7q8r9s0t",
      "nameV2": "Read CSV Content",
      "type": "script",
      "outputFormat": "text",
      "script": "\nconst fs = require('fs');\n\nmodule.exports = async function() {\n  try {\n    // Get CSV file path from previous step\n    const csvPath = data[\"4d5e6f7g-8h9i-0j1k-2l3m-4n5o6p7q8r9s\"];\n    \n    if (!csvPath) {\n      throw new Error(\"CSV file path not found\");\n    }\n    \n    // Read CSV file content\n    const csvContent = fs.readFileSync(csvPath, 'utf-8');\n    \n    if (!csvContent) {\n      throw new Error(\"CSV file is empty\");\n    }\n    \n    return csvContent;\n    \n  } catch (error) {\n    throw new Error(`Failed to read CSV: ${error.message}`);\n  }\n}\n",
      "nodeIds": ["4d5e6f7g-8h9i-0j1k-2l3m-4n5o6p7q8r9s"]
    }
  ],
  "results": [
    {
      "type": "plain_text",
      "fields": [],
      "items": { "content": "1a2b3c4d-5e6f-7g8h-9i0j-1k2l3m4n5o6p" }
    },
    {
      "type": "local_file",
      "fields": [],
      "items": { "content": "4d5e6f7g-8h9i-0j1k-2l3m-4n5o6p7q8r9s" }
    },
    {
      "type": "html",
      "fields": ["content"],
      "items": { "content": "5e6f7g8h-9i0j-1k2l-3m4n-5o6p7q8r9s0t" },
      "metadata": { "templateId": "qa-page-style-4" }
    }
  ],
  "version": "1.0",
  "metadata": {
    "author": "Updated with CSV export",
    "createdAt": "2026-01-30T07:41:49.140Z"
  },
  "frequency": "-1",
  "workflowConnects": []
}
